# 학습률 스케줄링
- 정의
  - 학습률을 너무 크게 잡으면 발산할 수 있으며, 너무 작게 잡으면 최적점에 수렴하는데 오랜 시간이 걸린다.
  - 큰 학습률로 시작하고 학습 속도가 느려질 때 학습률을 낮추면 최적의 고정 학습률보다 좋은 솔루션을 더 빨리 발견할 수 있다.
- 방법
  - 학습값을 크게 설정했다가, 학습의 진행과 함께 줄여나가기
    - 학습률을 파라미터 업데이트 수에 비례하여 감소하도록 스케줄링
    - 특정 업데이트가 지나면 감소하는 비율도 변경하기도 한다.
    - 최적의 방법은 아니며, 오히려 배치 사이즈를 늘리는 것이 좋다는 연구 결과도 있다.
  - 층마다 다른 학습률을 사용
    - 각 층의 가중치 업데이트 속도가 비슷하도록 가중치를 설정하는 것이 좋다.
    - 로지스틱 함수처럼 출력값이 한정적인 활성화 함수를 사용할 때, 중요하다.
    - 출력층과 가까운 층에서는 학습률을 작게, 입력층과 가까운 층에서는 학습률을 크게 한다.
  - 학습률을 자동적으로 결정 (Adagrad)
    - 드물게 나타나는 기울기의 성분을 자주 나타나는 기울기의 성분보다 중시하여 파라미터 업데이트한다.

## 거듭제곱 기반 스케줄링 (Power Scheduling)
- 학습이 진행될수록 학습률은 감소한다.
- 처음엔 Local Minimum이 멀리 있을테니 긴 보폭으로 다가가다가 거의 다 왔을 때는 보폭을 줄여 Local Minimun에 세심하게 가까이 가려는 것이다.

## 지수 기반 스케줄링 (Exponential Scheduling)
- 학습률을 반복 횟수 t에 대한 아래 함수로 정의한다.
- s 스텝마다 학습률이 10배씩 감소한다.
  
<img width="175" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/e65706e5-501f-44c1-bcdb-8aa13a364845">


## 구간별 고정 스케줄링 (Piecewise Constant Scheduling)
- 일정 횟수의 에포크 동안 일정한 학습률을 사용하고
- 그 다음 또 다른 횟수의 에포크 동안은 더 작은 학습률을 사용하는 방식이다.

## 성능 기반 스케줄링 (Performance Scheduling)
- 매 N번의 스텝마다 Validation Loss를 측정하고, 이것이 줄어들지 않으면 i배만큼 학습률을 감소시킨다.

## 1사이클 스케줄링 (1cycle Scheduling)
- 훈련 절반 동안 초기 학습률 𝜂0을 선형적으로 𝜂1까지 증가시킨다. 나머지 절반 동안 선형적으로 𝜂0까지 다시 감소시킨다. 마지막 몇 번의 포크는 소수점 몇 째 자리까지 줄인다.

---
## 추가 개념 정리

### 학습률
- 정의
  - 경사하강법에서 파라미터를 업데이트하는 정도를 조절하기 위한 변수이다.
  - 적절한 모델과 가중치 초기값을 설정했음에도, 학습률에 따라서 모델의 학습이 달라질 수 있다.

### 경사하강법 (Gradient Descent; GD)
- 정의
  - 함수의 기울기를 구하고 경사의 반대 방향으로 계속 이동시켜 극값에 이를 때까지 반복하는 것이다.
  - 머신러닝 모델의 옵티마이저(주어진 데이터에 맞게 모델 파라미터들을 최적화시켜주는 역할)의 한 종류이다.
    - 경사하강법 외에도 여러 옵티마이저가 존재한다.
    - 하지만 결국 다른 옵티마이저들도 경사하강법에서 새로운 개념을 추가하거나 단점을 보완해 나가면서 만들어졌다.
- 사용하는 이유
  - 인공지능은 손실 함수를 통하여 자신의 파라미터를 검증한다.
  - 특정 파라미터를 통해 나온 손실 함수 값이 가장 낮은 곳이 바로 **최적의 파라미터** 라고 할 수 있다.
  - Q. 그러면 그냥 손실 함수 미분해서 최소 & 최대 값을 찾으면 되지 않을까?
    - 실제 우리가 마주치는 함수들은 간단한 함수가 아니라 복잡하고, 비선형적인 함수가 대부분이므로 미분을 통해 그 값을 계산하기 어려운 경우가 많다.
    - 미분을 구현하는 과정보다 경사 하강법을 구현하여 최솟값을 찾는 것이 실질적으로 더 효율적이다.
- 문제점
  - 적절한 학습률 (learing rate)
    - 학습률이 높으면,
      - 한 번에 이동하는 거리가 커지므로 최적값에 빨리 수렴할 수 있다는 장점이 있지만,
      - 너무 크게 설정하면 최적값에 수렴하지 못하고 다른 곳으로 발산하는 현상이 나타날 수 있다.
    - 학습률이 낮으면,
      - 발산하지 않는 대신 최적값에 수렴하는 시간이 오래 걸릴 수 있다.
    - 따라서, 학습률을 적절히 조정하는 것이 매우 중요하다.
  - local minimum
    - 우리는 전역 (global) 최솟값을 찾고 싶지만, 어떤 이유로 인해 지역 (local) 최솟값에 빠지는 경우 탈출하지 못하고 거기로 수렴해버릴 수 있다.
<img width="300" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/451ed3d5-e265-4610-a7d2-4911b9660a23">
      
### 손실 함수
- 정의
  - 머신러닝에서는 오차를 손실(Loss) 또는 비용(Cost), 오차(Error)를 정의한 함수를 손실 함수 (Loss function) 또는 비용 함수 (Cost function), 오차 함수 (Error function) 이라고 한다.
  - 머신러닝 알고리즘은 손실 함수의 값을 최소화 하는 방향으로 학습하는 것이 목적이다.

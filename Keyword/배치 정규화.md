# Batch normalization (배치 정규화)
## 등장 배경
### Batch
- 경사 하강법인 GD에서는 gradient를 한 번에 업데이트하기 위해 모든 학습 데이터를 사용한다.
- 즉, 학습 데이터 전부를 넣어서 gradient를 다 구하고 그 모든 gradient를 평균해서 한 번에 모델 업데이트를 한다.
- ❌ 이런 방식으로 하면 대용량의 데이터를 한 번에 처리하지 못한다.
- 💡 그래서 데이터를 `batch` 단위로 나눠서 학습을 하는 방법이 일반적이게 되었다.
<img width="500" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/54c49e98-7e9c-4ad7-9038-993e1387f8fc">

- 그래서 SGD (Stochastic Gradient Descent)를 사용한다.
- SDG에서는 gradient를 한 번 업데이트 하기 위하여 `일부의 데이터`만을 사용한다. 즉, `batch`의 size만큼만 사용한다.

### Internal Covariate Shift
<img width="500" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/67ca83c2-d70a-48f6-8c09-37ddcdd97fa2">

- ❌ `Batch` 단위로 학습하게 되면 문제점이 있는데, 이것이 바로 ICS (Internal Covariate Shift) 이다.
- ICS는 **학습 과정에서 계층 별로 입력의 데이터 분포가 달라지는 현상**이다.
- 각 계층에서 입력으로 feature를 받게 되고 그 feature는 convolution이나 위와 같이 fully connected 연산을 거친 뒤, activation function을 적용하게 된다.
- 그러면 **연산 전/후의 데이터 간 분포가 달라질 수 있다.**
- 이와 유사하게 Batch 단위로 학습을 하게 되면 **Batch 단위 간에 데이터 분포의 차이**를 발생할 수 있다.
- 즉, Batch 간의 데이터가 상이하다고 말할 수 있는데 위에서 말한 `Internal Covariate Shift` 문제다.
- 💡 이 문제를 개선하기 위한 개념이 **배치 정규화 (Batch Normalization)** 이다.

## 정의
<img width="500" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/9616d8a7-1b27-4e3c-9f5e-2d7e1944c9d3">

- 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 **각 배치별로 평균과 분산을 이용해 정규화**하는 것을 뜻한다.
- 여기서 중요한 것은 학습 단계와 추론 단계에서 배치 정규화가 조금 다르게 적용되어야 된다는 것이다.
- 인공 신경망에서 사용되는 기법 중 하나로, 학습 과정에서 각 층의 입력을 정규화하는 방법이다. 이를 통해 학습의 안정성과 속도를 향상시킬 수 있다.
- 일반적으로 심층 신경망에서 발생하는 내부 공변량 변화(Internal Covariate Shift) 문제를 완화하는 데 사용된다. 내부 공변량 변화란, 신경망의 각 층을 지날 때마다 입력 데이터의 분포가 변하는 현상을 의미한다. 이러한 변화는 학습 과정을 어렵게 만들 수 있다.
- 각 미니배치의 입력을 평균과 분산으로 정규화한다. 이를 통해 입력의 분포를 안정화하고, 층 간의 상호 작용을 원활하게 한다. 또한, 배치 정규화는 각 층마다 학습 가능한 매개변수를 도입하여 층의 출력 범위를 조절하는 역할도 수행한다.

## 레퍼런스
[1] [JINSOL KIM](https://gaussian37.github.io/dl-concept-batchnorm/?source=post_page-----6894b3a46844--------------------------------)



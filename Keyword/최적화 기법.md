# 최적화 기법
<img width="300" alt="image" src="https://github.com/scottXchoo/Deep_Learning_Deep_Dive/assets/107841492/ddc3f9c9-dc21-4c54-89fc-b0c1722ef2fd">

모든 최적화 기법은 GD (혹은 SGD - Stochastic Gradient Descent)에서 출발한다.

### Momentum
- GD의 단점
  - 기울기 0인 점을 잘 탈출하지 못한다.
  - 너무 훈련이 느리다.
- 정의
  - 변수가 가던 방향으로 계속 가도록 하는 속도(velocity)항을 추가하는 것
  - 바른 방향으로 가고 있다면 점점 더 속도가 빨라지게 되어 더 빨리 훈련이 될 수 있다.
  - 현재 기울기가 0인 안장점이더라도 속도가 있으니 계속 이동해 안장점을 더 잘 탈출할 수 있게 된다.

### NAG (Nestreove Accelerated Gradient)
- Momentum의 단점
  - 실제 local minimum 등에 가까워졌지만 이를 지나쳤을 때
  - 계속해서 local minimun과 반대 방향으로 가고자 하는 관성이 있으므로 수렴을 하는 데에 조금 더 오랜 시간이 걸릴 수 있다.
- 정의
  - Momentum과 비슷하지만 약간의 variation이 추가된 상태
  - local minimum를 지나친다고 하더라도 현 시점에서 한 스텝을 더 밟은 그래디언트를 반영하기 때문에 minima를 지나치고자 하는 관성을 조금 더 빠르게 탈출할 수 있다.

### Adagrad
- 정의
  - learing rate 조절을 통해 뉴럴넷 내에서 많이 변화한 파라미터들에 대해서는 적게 변화를 시키고
  - 그렇지 않은 파라미터들에 대해서는 많이 변화시키는 방법

### Adadelta
- Adagrad의 단점
  - training이 길어질 때, gt 앞의 항의 분모에 있는 Gt(t시점까지의 그래디언트 제곱합)가 지속적으로 커지면서(property of monotonic increasing) 0에 가까워져 학습이 이루어지지 않는다.
- 정의
  - Adagrad에서는 모든 timestep에 대해서 Gt를 계산했다면,
  - Adadelta에서는 기간(window)을 설정해놓고 그 window size만큼의 기간에 해당하는 Gradient만 반영을 하겠다는 것이다.
  - 다만, window size만큼의 기간에 해당하는 gradient를 저장해두어야 하기 때문에 공간적으로 비효율적이다.
  - 이러한 비효율성을 방지하기 위해 EMA(Exponential Moving Average)를 사용한다.
  - learing rate가 따로 존재하지 않아 우리가 바꿀 수 있는 부분이 많이 없어 잘 활용되지 않는다.

### RMSprop
- Adadelta의 단점
  - 학습률이 없다.
- 정의
  - Adadelta와 마찬가지로 gradient squares의 지수이동평균을 사용하지만, 여기서는 학습률인 η을 도입해서 이를 Gt의 square root로 나눠주는 방식이다.
  - 즉, Adadelta에서 다시 학습률을 도입한 최적화 방법론이다.

### Adam (Adaptive Moment Estimation)
- 정의
  - 가장 무난하게 사용된다.
  - 과거의 그래디언트와 과거의 squared gradient를 모두 이용한다.
  - Adam 알고리즘은 adaptive learing rate 방법과 momentum을 효과적으로 결합한 알고리즘이다.
